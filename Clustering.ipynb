{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKUxguHZqbSWfA29PmnS5U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veenasnair18/Python-tutorial-using-Jupyter-Notebook-/blob/master/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCID_YZ60KQR"
      },
      "source": [
        "Clustering is a Machine Learning technique(Unsupervised Learning) that involves the grouping of data points. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. (https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n",
        "\n",
        "Unfortunalty there are issues with the current version of TensorFlow and the implementation for KMeans. This means we cannot use KMeans without writing the algorithm from scratch. We aren't quite at that level yet, so we'll just explain the basics of clustering for now.\n",
        "\n",
        "Basic Algorithm for K-Means.\n",
        "\n",
        "> Step 1: Randomly pick K points to place K centroids\n",
        "Step 2: Assign all the data points to the centroids by distance. The closest centroid to a point is the one it is assigned to.\n",
        "Step 3: Average all the points belonging to each centroid to find the middle of those clusters (center of mass). Place the corresponding centroids into that position.\n",
        "Step 4: Reassign every point once again to the closest centroid.\n",
        "Step 5: Repeat steps 3-4 until no point changes which centroid it belongs to.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZumL7hshrI1"
      },
      "source": [
        "# Hidden Markov Model\n",
        "The Hidden Markov Model is a finite set of states, each of which is associated with a (generally multidimensional) probability distribution []. Transitions among the states are governed by a set of probabilities called transition probabilities.\" (http://jedlik.phy.bme.hu/~gerjanos/HMM/node4.html)\n",
        "\n",
        "A hidden markov model works with probabilities to predict future events or states. In this section we will learn how to create a hidden markov model that can predict the weather.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odgYj8jc0Mij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a01c73-482d-4c2d-a285-ae6064d8b908"
      },
      "source": [
        "!pip install tensorflow_probability==0.8.0rc0 --user --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_probability==0.8.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/63/f54ce32063abaa682d779e44b49eb63fcf63c2422f978842fdeda794337d/tensorflow_probability-0.8.0rc0-py2.py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow_probability==0.8.0rc0) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_probability==0.8.0rc0) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow_probability==0.8.0rc0) (1.19.5)\n",
            "Collecting cloudpickle==1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/24/fb/4f92f8c0f40a0d728b4f3d5ec5ff84353e705d8ff5e3e447620ea98b06bd/cloudpickle-1.1.1-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: gym 0.17.3 has requirement cloudpickle<1.7.0,>=1.2.0, but you'll have cloudpickle 1.1.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cloudpickle, tensorflow-probability\n",
            "Successfully installed cloudpickle-1.1.1 tensorflow-probability-0.8.0rc0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czIfXk2Hh9Gr"
      },
      "source": [
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSdY_sH_iFU7"
      },
      "source": [
        "tfd = tfp.distributions  # making a shortcut for later on\n",
        "initial_distribution = tfd.Categorical(probs=[0.2, 0.8])  # Refer to point 2 above\n",
        "transition_distribution = tfd.Categorical(probs=[[0.5, 0.5],\n",
        "                                                 [0.2, 0.8]])  # refer to points 3 and 4 above\n",
        "observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])  # refer to point 5 above\n",
        "\n",
        "# the loc argument represents the mean and the scale is the standard devitation"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUmo5NoViJtt"
      },
      "source": [
        "model = tfd.HiddenMarkovModel(\n",
        "    initial_distribution=initial_distribution,\n",
        "    transition_distribution=transition_distribution,\n",
        "    observation_distribution=observation_distribution,\n",
        "    num_steps=7)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evE3oHr_iNPy",
        "outputId": "87f13ba3-e46a-460a-d33d-a94515ac8028"
      },
      "source": [
        "mean = model.mean()\n",
        "\n",
        "# due to the way TensorFlow works on a lower level we need to evaluate part of the graph\n",
        "# from within a session to see the value of this tensor\n",
        "\n",
        "# in the new version of tensorflow we need to use tf.compat.v1.Session() rather than just tf.Session()\n",
        "with tf.compat.v1.Session() as sess:  \n",
        "  print(mean.numpy())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[12.       11.1      10.83     10.748999 10.724699 10.71741  10.715222]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFxWMG4miQmx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}